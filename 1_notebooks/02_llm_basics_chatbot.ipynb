{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49caf25d",
   "metadata": {},
   "source": [
    "# Session 2 – Notebook 2: LLM Basics\n",
    "\n",
    "**Objectives:**\n",
    "- Learn how to use Hugging Face models with the `pipeline`.\n",
    "- Compare LLM responses with rule-based chatbot answers.\n",
    "- Understand the advantages and limitations of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63d4529",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LLM CHATBOT (using Hugging Face pipeline)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Import the Hugging Face pipeline utility\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load a small instruction-following model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# flan-t5-small is good for Q&A and simple instructions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m gen \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# LLM CHATBOT (using Hugging Face pipeline)\n",
    "\n",
    "# Import the Hugging Face pipeline utility\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small instruction-following model\n",
    "# flan-t5-small is good for Q&A and simple instructions\n",
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77138780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hey\n",
      "LLM Bot: Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey! Hey!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Greeting\n",
    "\n",
    "user_input = \"Hey\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "# Ask the model to generate a response (limit to 30 new tokens for short answers)\n",
    "response = gen(user_input, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e326238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is your name?\n",
      "LLM Bot: samuel wilson\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Open-ended question\n",
    "\n",
    "user_input = \"what is your name?\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "response = gen(user_input, max_new_tokens=30)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f72844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me a short story about a cat who learns to code.\n",
      "LLM Bot: The cat is learning to code. He learns to play a game. He learns to play a game. He learns to play a game. He learns to play a game. He learns to play a game. He learns to play a game\n"
     ]
    }
   ],
   "source": [
    "# Example 3: A creative question (not possible in rule-based bot)\n",
    "\n",
    "user_input = \"Tell me a short story about a cat who learns to code.\"\n",
    "print(\"User:\", user_input)\n",
    "\n",
    "response = gen(user_input, max_new_tokens=60)\n",
    "\n",
    "print(\"LLM Bot:\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ac7fb",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    "- Did the LLM understand \"what is your name?\" even though we didn’t code a rule?  \n",
    "- How does this compare to the rule-based bot from Notebook 1?  \n",
    "- What kind of tasks are **impossible** for a rule-based bot but possible for an LLM?  \n",
    "- What are the risks of using an LLM (e.g., making things up)?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Chatbot (type 'bye' to exit)\n",
      "Bot: Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi! Hi!\n",
      "Bot: hta\n",
      "Bot: if you are a snob, you are not a snob.\n",
      "Bot: I am not a snob. I am not a snob. I am not a snob. I am not a snob. I am not a \n",
      "Bot: Because it's a good idea to have a conversation with a friend.\n",
      "Bot: nagarpur\n",
      "Bot: sarajevo\n",
      "Bot: Delhi is the capital of India.\n",
      "Bot: sarajevo\n",
      "Bot: Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello! Hello!\n",
      "Bot: edward wilson\n",
      "Bot: samuel wilson\n"
     ]
    }
   ],
   "source": [
    "# FULL LLM CHATBOT (CLI style with loop)\n",
    "\n",
    "print(\"LLM Chatbot (type 'bye' to exit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() == \"bye\":\n",
    "        print(\"Bot: Goodbye, see you soon!\")\n",
    "        break\n",
    "\n",
    "    if user_input:\n",
    "        # Generate a response from the model\n",
    "        response = gen(user_input, max_new_tokens=50)\n",
    "        print(\"Bot:\", response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
